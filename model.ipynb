{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a8509e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58a4a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module): \n",
    "    def __init__(self, d_model: int, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding = nn.Embedding(vocabulary_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # formula from Section 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "683f28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference Section 3.5. \n",
    "Positional encodings embed the positional information of the tokens. \n",
    "This allows for the preservation of sequences (as such, the context) eg: \n",
    "\"It is cheaper to book now\"\n",
    "\"Can you return the book?\"\n",
    "We need sentence_len vectors that encode d_model dimensions. \n",
    "\"\"\"\n",
    "\n",
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, sentence_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.sentence_len = sentence_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        posititional_matrix = torch.zeros(sentence_len, d_model)\n",
    "        position = torch.arange(0, sentence_len, dtype=torch.float32).unsqueeze(1) # Creates 2 dimensional indexed matrix from 0 to sentence_len - 1 (inclusive) as rows\n",
    "        denominator = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model)) # This is a computational simplication with logarithms. The denominator in Section 3.5 is 10000^(2i/d_model)\n",
    "        posititional_matrix[:,0::2] = torch.sin(position * denominator) \n",
    "        posititional_matrix[:,1::2] = torch.cos(position * denominator)\n",
    "        posititional_matrix = posititional_matrix.unsqueeze(0)\n",
    "        self.register_buffer(\"positional_matrix\", posititional_matrix) # This is a fixed tensor. Hence, registering as buffer saves this alongside model parameters. \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_matrix[:, :x.shape[1], :].requires_grad_(False) #:x.shape[1] is the sentence length, we add the positional encodings to the inputs. We do not need to track gradients since positional matrix is not a training param\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5a57a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To allow for parallel normalization, we use layer normalization.\n",
    "This normalizes the data by the colkumn \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, epsilon:float = 10**-6): #Epsilon to avoid div by 0 errors\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features)) #bias param\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim=True) # IMP: keepdim parameter needed as default Pytorch behavior removes dimension along which mean or std calculated\n",
    "        standard_dev = x.std(dim = -1, keepdim=True) \n",
    "        return self.alpha * ((x - mean)/ (standard_dev + self.epsilon)) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed775539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi head attention implementation: \n",
    "Reference Section 3.2.1 and 3.2.2\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, dropout : float):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0, \"d_model needs to be divisible by heads parameter\"\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model # Refer to 3.2.2 for dimensional specifications\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # bias term is excluded in the original paper \n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout): \n",
    "        d_k = query.shape[-1] #d_k value\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k) #swaps the sentence length and d_k dimensions, formula from the scalar dot attention section\n",
    "        if mask is not None: #Masking ensures that the tokens we are using as paddings do not incorrectly influence our calculations\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) \n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        if dropout is not None: \n",
    "            attention_scores = dropout(attention_scores)\n",
    "        return torch.matmul(attention_scores, value), attention_scores\n",
    "\n",
    "    def forward(self, q, v, k, x, mask):\n",
    "        query = self.w_q(q) \n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        \"\"\"\n",
    "        We need to reshape the query, key, value tensors. \n",
    "        Respectively, we need to divide acros head number of tensors then change the dimensions such that we can calulcate attention (dimension details in Section 3.2.1)\n",
    "        \"\"\"\n",
    "        query = query.view(query.shape[0], query.shape[1], self.heads, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.heads, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.heads, self.d_k).transpose(1, 2)\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout) \n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k) # Reassembles data\n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d89ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Residual connections allow for imp information to be carried across layers. \n",
    "Without these, training would be harder as gradients in deeper layers would stuggle being propagated.\n",
    "^^ Vanishing gradient problem\n",
    "\n",
    "Explanation of vanishing gradient problem + Residual Layers: \n",
    "https://medium.com/analytics-vidhya/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c\n",
    "\n",
    "Also in section 3.1\n",
    "\"\"\"\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "        def __init__(self, features: int, dropout: float):\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "        def forward(self, x, layer):\n",
    "            return x + self.dropout(layer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a1e5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Section 3.3 Feed-Forward Network Implementation\n",
    "\"\"\"\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.first_linear = nn.Linear(d_model, d_ff) \n",
    "        self.dropout = nn.Dropout(dropout) # Specifies dropout in between\n",
    "        self.second_linear = nn.Linear(d_ff, d_model) #Ref formula, inner dimensions need to be the same for first and second linear layer\n",
    "    def forward(self, x):\n",
    "        return self.second_linear(self.dropout(self.first_linear(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af3bfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Architecture here is derived from Figure 1. \n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb3d4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention: MultiHeadAttention, feed_forward: FeedForward, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "    def forward(self, x, encoder_masks):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, encoder_masks))  # Reference Figure 1 (bottom residual connection)\n",
    "        # lambda needed in the above function to delay the function being called (we are passing the function call and not calling the function right now)\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block) # Reference Figure 1 (top residual connection)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6610ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) \n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, masked_self_attention: MultiHeadAttention, cross_attention: MultiHeadAttention, feed_forward: FeedForward, dropout: float):\n",
    "        super().__init__()\n",
    "        self.masked_self_attention = masked_self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)]) # Ref Figure 1, three residual connections needed\n",
    "    def forward(self, x, encoder_output, encoder_masks, decoder_masks):\n",
    "        x = self.residual_connections[0](x, lambda x: self.masked_self_attention(x, x, x, decoder_masks))\n",
    "        x = self.residual_connections[1](x,  lambda x: self.cross_attention(x, encoder_output, encoder_output, encoder_masks))\n",
    "        x = self.residual_connections[2](x, self.feed_forward)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e38c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self,  d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.output = F.softmax(nn.Linear(d_model, vocab_size))\n",
    "    def forward(self, x):\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4028d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, source_embedding: InputEmbeddings, target_embeddings: InputEmbeddings, source_positional: PositionalEmbeddings, target_positional: PositionalEmbeddings, output_layer: OutputLayer):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embedding = source_embedding\n",
    "        self.target_embedding = target_embeddings\n",
    "        self.source_positional = source_positional\n",
    "        self.target_positional = target_positional\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        source = self.source_positional(self.source_embedding(source))\n",
    "        return self.encoder(source, source_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, source_mask: torch.Tensor, target: torch.Tensor, target_mask: torch.Tensor):\n",
    "            # (batch, seq_len, d_model)\n",
    "        target = self.target_positional(self.target_embedding(target))\n",
    "        return self.decoder(target, encoder_output, source_mask, target_mask)\n",
    "        \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.output_layer(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(source_vocab_size: int, target_vocab_size: int, source_sentence_len: int, target_sentence_len: int, d_model: int = 512, N: int = 6, h:int=8, dropout: float= 0.1, d_ff: int = 2048):\n",
    "    source_embedding = InputEmbeddings(d_model, source_vocab_size)\n",
    "    target_embedding = InputEmbeddings(d_model, target_vocab_size)\n",
    "    source_positional = PositionalEmbeddings(d_model, source_sentence_len, dropout)\n",
    "    target_positional = PositionalEmbeddings(d_model, target_sentence_len, dropout)\n",
    "    encoders= []\n",
    "    for i in range(N):\n",
    "        self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, self_attention_block, feed_forward, dropout)\n",
    "        encoders.append(encoder_block)\n",
    "    decoders = []\n",
    "    for i in range(N):\n",
    "        self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        cross_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "        feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        decoder_block = EncoderBlock(d_model, self_attention_block, cross_attention_block, feed_forward, dropout)\n",
    "        decoders.append(decoder_block)\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoders))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder))\n",
    "\n",
    "    output_layer = OutputLayer(d_model, target_vocab_size)\n",
    "    transformer = Transformer(encoder, decoder, source_embedding, target_embedding, source_positional, target_positional, output_layer)\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f38799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6f524e4",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65d06c",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=ISNdQcPhsts\n",
    "https://arxiv.org/pdf/1706.03762"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
